{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Detection\n",
    "\n",
    "In this project you will predict fraudulent credit card transactions with the help of Machine learning models. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Initial Basic Steps\n",
    "1. Importing necessary libraries\n",
    "2. Loading data\n",
    "3. Observe basic structure of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load data and observe basic structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observe the different feature type present in the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will observe the distribution of our classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=df['Class'].value_counts()\n",
    "normal_share=classes[0]/df['Class'].count()*100\n",
    "fraud_share=classes[1]/df['Class'].count()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].bar(x=[\"normal_share\", \"fraud_share\"], height=classes)\n",
    "axs[0].set_title(\"Counts\")\n",
    "axs[1].bar(x=[\"normal_share\", \"fraud_share\"], height=[normal_share, fraud_share])\n",
    "axs[1].set_title(\"Percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to observe the distribution of classes with time\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(df.Time, df.Class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot to observe the distribution of classes with Amount\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.scatter(df.Amount, df.Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df=df.drop(\"Time\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df.Class #class variable\n",
    "X=df.drop(\"Class\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, stratify=y, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preserve X_test & y_test to evaluate on the test data once you build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(y))\n",
    "print(np.sum(y_train))\n",
    "print(np.sum(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the distribution of a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the histogram of a variable from the dataset to see the skewness\n",
    "fig,axs=plt.subplots(6,5)\n",
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "        try:\n",
    "            axs[i,j].hist(X_train[X_train.columns[5*i+j]], bins=100)\n",
    "            axs[i,j].set_title(X_train.columns[5*i+j])\n",
    "        except:\n",
    "            pass\n",
    "fig.set_size_inches(22,24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`We can see that there are many variables which have very high skewness so lets find the list of variable which have more than 0.5 skew and less than -0.5 skew`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# See the features with more than 0.5 or less that -0.5 skew\n",
    "skew=X_train.skew()\n",
    "# Take absolute value of skew and then get all the columns whose absolute value of skew is more than 0.5\n",
    "skewed=skew[np.abs(skew)>0.5]\n",
    "print(skewed)\n",
    "print(\"The total number of features with skewness more than 0.5 or less than -0.5 are \", len(skewed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is skewness present in the distribution use:\n",
    "- <b>Power Transformer</b> package present in the <b>preprocessing library provided by sklearn</b> to make distribution more gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Apply : preprocessing.PowerTransformer(copy=False) to fit & transform the train & test data\n",
    "power_trans=preprocessing.PowerTransformer(copy=False)\n",
    "power_trans.fit_transform(X_train)\n",
    "power_trans.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the histogram of a variable from the dataset again to see the result \n",
    "fig,axs=plt.subplots(6,5)\n",
    "for i in range(6):\n",
    "    for j in range(5):\n",
    "        try:\n",
    "            axs[i,j].hist(X_train[X_train.columns[5*i+j]], bins=100)\n",
    "            axs[i,j].set_title(X_train.columns[5*i+j])\n",
    "        except:\n",
    "            pass\n",
    "fig.set_size_inches(22,24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now again See the features with more than 0.5 or less that -0.5 skew after power transform\n",
    "skew=X_train.skew()\n",
    "# Take absolute value of skew and then get all the columns whose absolute value of skew is more than 0.5\n",
    "skewed=skew[np.abs(skew)>0.5]\n",
    "print(skewed)\n",
    "print(\"The total number of features with skewness more than 0.5 or less than -0.5 are \", len(skewed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Building\n",
    "- Build different models on the imbalanced dataset and see the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn import linear_model #import the package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### perfom cross validation on the X_train & y_train to create:\n",
    "- X_train_cv\n",
    "- X_test_cv \n",
    "- y_train_cv\n",
    "- y_test_cv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Cross validation done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross validation manually\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=100)\n",
    "accuracy_scores =[]\n",
    "recall_scores = []\n",
    "precission_scores = []\n",
    "AUC_ROC=[]\n",
    "logistic_model=linear_model.LogisticRegression( random_state=100)\n",
    "for train_index, test_index in skf.split(X_train,y_train):\n",
    "    X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    logistic_model.fit(X_train_cv, y_train_cv)\n",
    "    predictions = logistic_model.predict(X_test_cv)\n",
    "    pred_proba= logistic_model.predict_proba(X_test_cv)[:,1]\n",
    "    accuracy_scores.append(metrics.accuracy_score(y_test_cv, predictions))\n",
    "    recall_scores.append(metrics.recall_score(y_test_cv, predictions))\n",
    "    precission_scores.append(metrics.precision_score(y_test_cv, predictions))\n",
    "    AUC_ROC.append(metrics.roc_auc_score(y_test_cv,pred_proba))\n",
    "print(\"The average accurcay score is\", np.mean(accuracy_scores))\n",
    "print(\"The average recall score is\", np.mean(recall_scores))\n",
    "print(\"The average precision score is\", np.mean(precission_scores))\n",
    "print(\"The average ROC AUC score is\", np.mean(AUC_ROC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "#perform hyperparameter tuning\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=100)\n",
    "logistic_model=linear_model.LogisticRegression(random_state=100, max_iter=1000)\n",
    "# define grid search\n",
    "params = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "# The best evaluation metric to evealuate the strength of a model is auc roc score\n",
    "grid_search = model_selection.GridSearchCV(estimator=logistic_model, param_grid=params, n_jobs=-1, cv=skf, scoring='roc_auc', error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "end=time.time()\n",
    "print(\"Time taken to run this is: \", round((end-start)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator\n",
    "grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See best score\n",
    "grid_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the optimum value of hyperparameters\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Logistic regression model using best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the model using these optimum hyperparametrs\n",
    "logistic_model=linear_model.LogisticRegression(penalty='l2', C=0.01, random_state=100, solver='liblinear',max_iter=1000)\n",
    "logistic_model.fit(X_train,y_train)\n",
    "# Find the probability of the target to be 1\n",
    "predict_proba= logistic_model.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predict_proba)\n",
    "# Plot the ROC curve to see which value of tpr and FPR will be a good option to choose\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that 0.85 will be a good value for TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph between tpr and thresholds to choose the threshold\n",
    "plt.plot(thresholds, tpr)\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"TPR vs Thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, the value of threshold correspond to TPR =0.85 should be around 0.06 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting the threshold to be 0.06 the classes of the target will be\n",
    "# Prediction for train data\n",
    "y_train_pred=logistic_model.predict_proba(X_train)[:,1]>0.07\n",
    "# Prediction for test data\n",
    "y_pred=predict_proba>0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 Logistic regression Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The recall score for the train data is: \", metrics.recall_score(y_train, y_train_pred))\n",
    "print(\"The precision score for the train data is: \", metrics.precision_score(y_train, y_train_pred))\n",
    "print(\"The recall score for the test data is: \", metrics.recall_score(y_test, y_pred))\n",
    "print(\"The precision score for the test data is: \", metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RandomForestClassifier Object\n",
    "rf = RandomForestClassifier()\n",
    "# See the hyperparameters of Random forest classifier\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 First try Random Grid Search to get a idea about the range of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\"]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Criterion for quality of a split\n",
    "criterion =[\"gini\", \"entropy\"]\n",
    "\n",
    "# Create Param grid\n",
    "grid_params={\"n_estimators\" : n_estimators, \"criterion\" : criterion, \"max_depth\" : max_depth, \"min_samples_split\" : min_samples_split, \"min_samples_leaf\" : min_samples_leaf, \"max_features\" : max_features}\n",
    "pprint(grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Create StratifiedK fold object\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True ,random_state=100)\n",
    "# Create RandomForestClassifier Object\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "# Creat Randomised grid search object\n",
    "random_grid_search = model_selection.RandomizedSearchCV(estimator=rf, param_distributions=grid_params, n_iter=20, scoring='roc_auc', n_jobs=-1, cv=skf, random_state=100, error_score=0)\n",
    "# Fit the model on train data\n",
    "random_grid_search.fit(X_train, y_train)\n",
    "\n",
    "end= time.time()\n",
    "print(\"Time taken to run this is: \", round((end-start)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator\n",
    "print(random_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best score\n",
    "print(random_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best params from random grid search\n",
    "print(random_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Grid Search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\"]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Criterion for quality of a split\n",
    "criterion =[\"gini\", \"entropy\"]\n",
    "\n",
    "# Create Param grid\n",
    "grid_params={\"n_estimators\" : n_estimators, \"criterion\" : criterion, \"max_depth\" : max_depth, \"min_samples_split\" : min_samples_split, \"min_samples_leaf\" : min_samples_leaf, \"max_features\" : max_features}\n",
    "pprint(grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Create StratifiedK fold object\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True ,random_state=100)\n",
    "# Create RandomForestClassifier Object\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "# Creat grid search object\n",
    "grid_search = model_selection.GridSearchCV(estimator=rf, param_distributions=grid_params, scoring='roc_auc', n_jobs=-1, cv=skf, random_state=100, error_score=0)\n",
    "# Fit the model on train data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "end= time.time()\n",
    "print(\"Time taken to run this is: \", round((end-start)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best score\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best params from random grid search\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Random Forest using best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the model using these optimum hyperparametrs\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "rf.fit(X_train,y_train)\n",
    "# Find the probability of the target to be 1\n",
    "predict_proba= rf.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predict_proba)\n",
    "# Plot the ROC curve to see which value of tpr and FPR will be a good option to choose\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph we can see that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph between tpr and thresholds to choose the threshold\n",
    "plt.plot(thresholds, tpr)\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"TPR vs Thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, the value of threshold correspond to TPR =0.85 should be around 0.06 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting the threshold to be 0.06 the classes of the target will be\n",
    "# Prediction for train data\n",
    "y_train_pred=rf.predict_proba(X_train)[:,1]>0.07\n",
    "# Prediction for test data\n",
    "y_pred=predict_proba>0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Random Forest Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The recall score for the train data is: \", metrics.recall_score(y_train, y_train_pred))\n",
    "print(\"The precision score for the train data is: \", metrics.precision_score(y_train, y_train_pred))\n",
    "print(\"The recall score for the test data is: \", metrics.recall_score(y_test, y_pred))\n",
    "print(\"The precision score for the test data is: \", metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Proceed with the model which shows the best result \n",
    "- Apply the best hyperparameter on the model\n",
    "- Predict on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ___  #initialise the model with optimum hyperparameters\n",
    "clf.fit(X_train, y_train)\n",
    "print --> #print the evaluation score on the X_test by choosing the best evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the important features of the best model to understand the dataset\n",
    "- This will not give much explanation on the already transformed dataset\n",
    "- But it will help us in understanding if the dataset is not PCA transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp = []\n",
    "for i in clf.feature_importances_:\n",
    "    var_imp.append(i)\n",
    "print('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\n",
    "print('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\n",
    "print('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n",
    "\n",
    "# Variable on Index-16 and Index-13 seems to be the top 2 variables\n",
    "top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\n",
    "second_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "np.random.shuffle(X_train_0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "\n",
    "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
    "            label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building with balancing Classes\n",
    "\n",
    "##### Perform class balancing with :\n",
    "- Random Oversampling\n",
    "- SMOTE\n",
    "- ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "- Build different models on the balanced dataset and see the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn import over_sampling #- import the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balance classes on X_train_cv & y_train_cv using Random Oversampling\n",
    "ro = over_sampling.RandomOverSampler(random_state=100)\n",
    "X_train_ro, y_train_ro = ro.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets see the class distribution of oversampled data\n",
    "classes=y_train_ro.value_counts()\n",
    "normal_share=classes[0]/len(y_train_ro)*100\n",
    "fraud_share=classes[1]/len(y_train_ro)*100\n",
    "# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].bar(x=[\"normal_share\", \"fraud_share\"], height=classes)\n",
    "axs[0].set_title(\"Counts\")\n",
    "axs[1].bar(x=[\"normal_share\", \"fraud_share\"], height=[normal_share, fraud_share])\n",
    "axs[1].set_title(\"Percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our data is oversampled properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "#perform hyperparameter tuning\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [ 100, 10, 0.1, 0.01]\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=100)\n",
    "logistic_model=linear_model.LogisticRegression(random_state=100, max_iter=1000)\n",
    "# define grid search\n",
    "params = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "# The best evaluation metric to evealuate the strength of a model is auc roc score\n",
    "grid_search = model_selection.GridSearchCV(estimator=logistic_model, param_grid=params, n_jobs=-1, cv=skf, scoring='roc_auc', error_score=0)\n",
    "grid_result = grid_search.fit(X_train_ro, y_train_ro)\n",
    "end=time.time()\n",
    "print(\"Time taken to run this is: \", round((end-start)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator\n",
    "grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See best score\n",
    "grid_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the optimum value of hyperparameters\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the model using these optimum hyperparametrs\n",
    "logistic_model=linear_model.LogisticRegression(penalty='l2', C=100, random_state=100, solver='newton-cg',max_iter=1000)\n",
    "logistic_model.fit(X_train_ro,y_train_ro)\n",
    "# Find the probability of the target to be 1\n",
    "predict_proba= logistic_model.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predict_proba)\n",
    "# Plot the ROC curve to see which value of tpr and FPR will be a good option to choose\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that 0.85 will be a good value for TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph between tpr and thresholds to choose the threshold\n",
    "plt.plot(thresholds, tpr)\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"TPR vs Thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, the value of threshold correspond to TPR =0.85 should be around 0.06 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting the threshold to be 0.999 the classes of the target will be\n",
    "# Prediction for train data\n",
    "y_train_pred=logistic_model.predict_proba(X_train_ro)[:,1]>0.999\n",
    "# Prediction for test data\n",
    "y_pred=predict_proba>0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The recall score for the train data is: \", metrics.recall_score(y_train_ro, y_train_pred))\n",
    "print(\"The precision score for the train data is: \", metrics.precision_score(y_train_ro, y_train_pred))\n",
    "print(\"The recall score for the test data is: \", metrics.recall_score(y_test, y_pred))\n",
    "print(\"The precision score for the test data is: \", metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Forest with Random OverSampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RandomForestClassifier Object\n",
    "rf = RandomForestClassifier()\n",
    "# See the hyperparameters of Random forest classifier\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 First try Random Grid Search to get a idea about the range of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\"]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Criterion for quality of a split\n",
    "criterion =[\"gini\", \"entropy\"]\n",
    "\n",
    "# Create Param grid\n",
    "grid_params={\"n_estimators\" : n_estimators, \"criterion\" : criterion, \"max_depth\" : max_depth, \"min_samples_split\" : min_samples_split, \"min_samples_leaf\" : min_samples_leaf, \"max_features\" : max_features}\n",
    "pprint(grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Create StratifiedK fold object\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True ,random_state=100)\n",
    "# Create RandomForestClassifier Object\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "# Creat Randomised grid search object\n",
    "random_grid_search = model_selection.RandomizedSearchCV(estimator=rf, param_distributions=grid_params, n_iter=20, scoring='roc_auc', n_jobs=-1, cv=skf, random_state=100, error_score=0)\n",
    "# Fit the model on train data\n",
    "random_grid_search.fit(X_train_ro, y_train_ro)\n",
    "\n",
    "end= time.time()\n",
    "print(\"Time taken to run this is: \", round((end-start)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator\n",
    "print(random_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best score\n",
    "print(random_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best params from random grid search\n",
    "print(random_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Grid Search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\"]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Criterion for quality of a split\n",
    "criterion =[\"gini\", \"entropy\"]\n",
    "\n",
    "# Create Param grid\n",
    "grid_params={\"n_estimators\" : n_estimators, \"criterion\" : criterion, \"max_depth\" : max_depth, \"min_samples_split\" : min_samples_split, \"min_samples_leaf\" : min_samples_leaf, \"max_features\" : max_features}\n",
    "pprint(grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Create StratifiedK fold object\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True ,random_state=100)\n",
    "# Create RandomForestClassifier Object\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "# Creat grid search object\n",
    "grid_search = model_selection.GridSearchCV(estimator=rf, param_distributions=grid_params, scoring='roc_auc', n_jobs=-1, cv=skf, random_state=100, error_score=0)\n",
    "# Fit the model on train data\n",
    "grid_search.fit(X_train_ro, y_train_ro)\n",
    "\n",
    "end= time.time()\n",
    "print(\"Time taken to run this is: \", round((end-start)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best score\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best params from random grid search\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Random Forest using best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the model using these optimum hyperparametrs\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "rf.fit(X_train_ro,y_train_ro)\n",
    "# Find the probability of the target to be 1\n",
    "predict_proba= rf.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predict_proba)\n",
    "# Plot the ROC curve to see which value of tpr and FPR will be a good option to choose\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph we can see that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph between tpr and thresholds to choose the threshold\n",
    "plt.plot(thresholds, tpr)\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"TPR vs Thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, the value of threshold correspond to TPR =0.85 should be around 0.06 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting the threshold to be 0.06 the classes of the target will be\n",
    "# Prediction for train data\n",
    "y_train_pred=rf.predict_proba(X_train_ro)[:,1]>0.07\n",
    "# Prediction for test data\n",
    "y_pred=predict_proba>0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Random Forest Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The recall score for the train data is: \", metrics.recall_score(y_train_ro, y_train_pred))\n",
    "print(\"The precision score for the train data is: \", metrics.precision_score(y_train_ro, y_train_pred))\n",
    "print(\"The recall score for the test data is: \", metrics.recall_score(y_test, y_pred))\n",
    "print(\"The precision score for the test data is: \", metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarly explore other algorithms on balanced dataset by building models like:\n",
    "- KNN\n",
    "- SVM\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the class distribution after applying SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "sm = over_sampling.SMOTE(random_state=0)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\n",
    "# Artificial minority samples and corresponding minority labels from SMOTE are appended\n",
    "# below X_train and y_train respectively\n",
    "# So to exclusively get the artificial minority samples from SMOTE, we do\n",
    "X_train_smote_1 = X_train_smote[X_train.shape[0]:].to_numpy()\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=[20,20])\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_smote_1[:X_train_1.shape[0], 0], X_train_smote_1[:X_train_1.shape[0], 1],\n",
    "            label='Artificial SMOTE Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets see the class distribution of oversampled data\n",
    "classes=y_train_smote.value_counts()\n",
    "normal_share=classes[0]/len(y_train_smote)*100\n",
    "fraud_share=classes[1]/len(y_train_smote)*100\n",
    "# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].bar(x=[\"normal_share\", \"fraud_share\"], height=classes)\n",
    "axs[0].set_title(\"Counts\")\n",
    "axs[1].bar(x=[\"normal_share\", \"fraud_share\"], height=[normal_share, fraud_share])\n",
    "axs[1].set_title(\"Percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "#perform hyperparameter tuning\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=100)\n",
    "logistic_model=linear_model.LogisticRegression(random_state=100, max_iter=1000)\n",
    "# define grid search\n",
    "params = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "# The best evaluation metric to evealuate the strength of a model is auc roc score\n",
    "grid_search = model_selection.GridSearchCV(estimator=logistic_model, param_grid=params, n_jobs=-1, cv=skf, scoring='roc_auc', error_score=0)\n",
    "grid_result = grid_search.fit(X_train_smote, y_train_smote)\n",
    "end=time.time()\n",
    "print(\"Time taken to run this is: \", round((end-start)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator\n",
    "grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See best score\n",
    "grid_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the optimum value of hyperparameters\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the model using these optimum hyperparametrs\n",
    "logistic_model=linear_model.LogisticRegression(penalty='l2', C=100, random_state=100, solver='newton-cg',max_iter=1000)\n",
    "logistic_model.fit(X_train_smote,y_train_smote)\n",
    "# Find the probability of the target to be 1\n",
    "predict_proba= logistic_model.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predict_proba)\n",
    "# Plot the ROC curve to see which value of tpr and FPR will be a good option to choose\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that 0.85 will be a good value for TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph between tpr and thresholds to choose the threshold\n",
    "plt.plot(thresholds, tpr)\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"TPR vs Thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, the value of threshold correspond to TPR =0.85 should be around 0.06 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting the threshold to be 0.999 the classes of the target will be\n",
    "# Prediction for train data\n",
    "y_train_pred=logistic_model.predict_proba(X_train_smote)[:,1]>0.999\n",
    "# Prediction for test data\n",
    "y_pred=predict_proba>0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The recall score for the train data is: \", metrics.recall_score(y_train_smote, y_train_pred))\n",
    "print(\"The precision score for the train data is: \", metrics.precision_score(y_train_smote, y_train_pred))\n",
    "print(\"The recall score for the test data is: \", metrics.recall_score(y_test, y_pred))\n",
    "print(\"The precision score for the test data is: \", metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Forest with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RandomForestClassifier Object\n",
    "rf = RandomForestClassifier()\n",
    "# See the hyperparameters of Random forest classifier\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 First try Random Grid Search to get a idea about the range of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\"]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Criterion for quality of a split\n",
    "criterion =[\"gini\", \"entropy\"]\n",
    "\n",
    "# Create Param grid\n",
    "grid_params={\"n_estimators\" : n_estimators, \"criterion\" : criterion, \"max_depth\" : max_depth, \"min_samples_split\" : min_samples_split, \"min_samples_leaf\" : min_samples_leaf, \"max_features\" : max_features}\n",
    "pprint(grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Create StratifiedK fold object\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True ,random_state=100)\n",
    "# Create RandomForestClassifier Object\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "# Creat Randomised grid search object\n",
    "random_grid_search = model_selection.RandomizedSearchCV(estimator=rf, param_distributions=grid_params, n_iter=20, scoring='roc_auc', n_jobs=-1, cv=skf, random_state=100, error_score=0)\n",
    "# Fit the model on train data\n",
    "random_grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "end= time.time()\n",
    "print(\"Time taken to run this is: \", round((end-start)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator\n",
    "print(random_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best score\n",
    "print(random_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best params from random grid search\n",
    "print(random_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Grid Search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\"]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Criterion for quality of a split\n",
    "criterion =[\"gini\", \"entropy\"]\n",
    "\n",
    "# Create Param grid\n",
    "grid_params={\"n_estimators\" : n_estimators, \"criterion\" : criterion, \"max_depth\" : max_depth, \"min_samples_split\" : min_samples_split, \"min_samples_leaf\" : min_samples_leaf, \"max_features\" : max_features}\n",
    "pprint(grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Create StratifiedK fold object\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True ,random_state=100)\n",
    "# Create RandomForestClassifier Object\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "# Creat grid search object\n",
    "grid_search = model_selection.GridSearchCV(estimator=rf, param_distributions=grid_params, scoring='roc_auc', n_jobs=-1, cv=skf, random_state=100, error_score=0)\n",
    "# Fit the model on train data\n",
    "grid_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "end= time.time()\n",
    "print(\"Time taken to run this is: \", round((end-start)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best score\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best params from random grid search\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Random Forest using best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the model using these optimum hyperparametrs\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "rf.fit(X_train_smote,y_train_smote)\n",
    "# Find the probability of the target to be 1\n",
    "predict_proba= rf.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predict_proba)\n",
    "# Plot the ROC curve to see which value of tpr and FPR will be a good option to choose\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph we can see that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph between tpr and thresholds to choose the threshold\n",
    "plt.plot(thresholds, tpr)\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"TPR vs Thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, the value of threshold correspond to TPR =0.85 should be around 0.06 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting the threshold to be 0.06 the classes of the target will be\n",
    "# Prediction for train data\n",
    "y_train_pred=rf.predict_proba(X_train_smote)[:,1]>0.07\n",
    "# Prediction for test data\n",
    "y_pred=predict_proba>0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Random Forest Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The recall score for the train data is: \", metrics.recall_score(y_train_smote, y_train_pred))\n",
    "print(\"The precision score for the train data is: \", metrics.precision_score(y_train_smote, y_train_pred))\n",
    "print(\"The recall score for the test data is: \", metrics.recall_score(y_test, y_pred))\n",
    "print(\"The precision score for the test data is: \", metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build models on other algorithms to see the better performing on SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the class distribution after applying ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from imblearn import over_sampling\n",
    "\n",
    "ada = over_sampling.ADASYN(random_state=0)\n",
    "X_train_adasyn, y_train_adasyn = ada.fit_resample(X_train, y_train)\n",
    "# Artificial minority samples and corresponding minority labels from ADASYN are appended\n",
    "# below X_train and y_train respectively\n",
    "# So to exclusively get the artificial minority samples from ADASYN, we do\n",
    "X_train_adasyn_1 = X_train_adasyn[X_train.shape[0]:].to_numpy()\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=[20,20])\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_adasyn_1[:X_train_1.shape[0], 0], X_train_adasyn_1[:X_train_1.shape[0], 1],\n",
    "            label='Artificial ADASYN Class-1 Examples')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.scatter(X_train_1[:, 0], X_train_1[:, 1], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], 0], X_train_0[:X_train_1.shape[0], 1], label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets see the class distribution of oversampled data\n",
    "classes=y_train_adasyn.value_counts()\n",
    "normal_share=classes[0]/len(y_train_adasyn)*100\n",
    "fraud_share=classes[1]/len(y_train_adasyn)*100\n",
    "# Create a bar plot for the number and percentage of fraudulent vs non-fraudulent transcations\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].bar(x=[\"normal_share\", \"fraud_share\"], height=classes)\n",
    "axs[0].set_title(\"Counts\")\n",
    "axs[1].bar(x=[\"normal_share\", \"fraud_share\"], height=[normal_share, fraud_share])\n",
    "axs[1].set_title(\"Percentage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression with ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "#perform hyperparameter tuning\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l2']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=100)\n",
    "logistic_model=linear_model.LogisticRegression(random_state=100, max_iter=1000)\n",
    "# define grid search\n",
    "params = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "# The best evaluation metric to evealuate the strength of a model is auc roc score\n",
    "grid_search = model_selection.GridSearchCV(estimator=logistic_model, param_grid=params, n_jobs=-1, cv=skf, scoring='roc_auc', error_score=0)\n",
    "grid_result = grid_search.fit(X_train_adasyn, y_train_adasyn)\n",
    "end=time.time()\n",
    "print(\"Time taken to run this is: \", round((end-start)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator\n",
    "grid_result.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See best score\n",
    "grid_result.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the optimum value of hyperparameters\n",
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the model using these optimum hyperparametrs\n",
    "logistic_model=linear_model.LogisticRegression(penalty='l2', C=100, random_state=100, solver='newton-cg',max_iter=1000)\n",
    "logistic_model.fit(X_train_adasyn,y_train_adasyn)\n",
    "# Find the probability of the target to be 1\n",
    "predict_proba= logistic_model.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predict_proba)\n",
    "# Plot the ROC curve to see which value of tpr and FPR will be a good option to choose\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that 0.85 will be a good value for TPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph between tpr and thresholds to choose the threshold\n",
    "plt.plot(thresholds, tpr)\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"TPR vs Thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, the value of threshold correspond to TPR =0.85 should be around 0.06 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting the threshold to be 0.06 the classes of the target will be\n",
    "# Prediction for train data\n",
    "y_train_pred=logistic_model.predict_proba(X_train_adasyn)[:,1]>0.999\n",
    "# Prediction for test data\n",
    "y_pred=predict_proba>0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The recall score for the train data is: \", metrics.recall_score(y_train_adasyn, y_train_pred))\n",
    "print(\"The precision score for the train data is: \", metrics.precision_score(y_train_adasyn, y_train_pred))\n",
    "print(\"The recall score for the test data is: \", metrics.recall_score(y_test, y_pred))\n",
    "print(\"The precision score for the test data is: \", metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Random Forest with ADAYSAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RandomForestClassifier Object\n",
    "rf = RandomForestClassifier()\n",
    "# See the hyperparameters of Random forest classifier\n",
    "pprint(rf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 First try Random Grid Search to get a idea about the range of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\"]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Criterion for quality of a split\n",
    "criterion =[\"gini\", \"entropy\"]\n",
    "\n",
    "# Create Param grid\n",
    "grid_params={\"n_estimators\" : n_estimators, \"criterion\" : criterion, \"max_depth\" : max_depth, \"min_samples_split\" : min_samples_split, \"min_samples_leaf\" : min_samples_leaf, \"max_features\" : max_features}\n",
    "pprint(grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Create StratifiedK fold object\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True ,random_state=100)\n",
    "# Create RandomForestClassifier Object\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "# Creat Randomised grid search object\n",
    "random_grid_search = model_selection.RandomizedSearchCV(estimator=rf, param_distributions=grid_params, n_iter=20, scoring='roc_auc', n_jobs=-1, cv=skf, random_state=100, error_score=0)\n",
    "# Fit the model on train data\n",
    "random_grid_search.fit(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "end= time.time()\n",
    "print(\"Time taken to run this is: \", round((end-start)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator\n",
    "print(random_grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best score\n",
    "print(random_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best params from random grid search\n",
    "print(random_grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Grid Search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = [\"auto\", \"sqrt\", \"log2\"]\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 7, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Criterion for quality of a split\n",
    "criterion =[\"gini\", \"entropy\"]\n",
    "\n",
    "# Create Param grid\n",
    "grid_params={\"n_estimators\" : n_estimators, \"criterion\" : criterion, \"max_depth\" : max_depth, \"min_samples_split\" : min_samples_split, \"min_samples_leaf\" : min_samples_leaf, \"max_features\" : max_features}\n",
    "pprint(grid_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# Create StratifiedK fold object\n",
    "skf=model_selection.StratifiedKFold(n_splits=5, shuffle=True ,random_state=100)\n",
    "# Create RandomForestClassifier Object\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "# Creat grid search object\n",
    "grid_search = model_selection.GridSearchCV(estimator=rf, param_distributions=grid_params, scoring='roc_auc', n_jobs=-1, cv=skf, random_state=100, error_score=0)\n",
    "# Fit the model on train data\n",
    "grid_search.fit(X_train_adasyn, y_train_adasyn)\n",
    "\n",
    "end= time.time()\n",
    "print(\"Time taken to run this is: \", round((end-start)/60, 2), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best estimator\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best score\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the best params from random grid search\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Random Forest using best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now train the model using these optimum hyperparametrs\n",
    "rf = RandomForestClassifier(random_state=100)\n",
    "rf.fit(X_train_adasyn, y_train_adasyn)\n",
    "# Find the probability of the target to be 1\n",
    "predict_proba= rf.predict_proba(X_test)[:,1]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predict_proba)\n",
    "# Plot the ROC curve to see which value of tpr and FPR will be a good option to choose\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph we can see that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot graph between tpr and thresholds to choose the threshold\n",
    "plt.plot(thresholds, tpr)\n",
    "plt.xlabel(\"Thresholds\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"TPR vs Thresholds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, the value of threshold correspond to TPR =0.85 should be around 0.06 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting the threshold to be 0.06 the classes of the target will be\n",
    "# Prediction for train data\n",
    "y_train_pred=rf.predict_proba(X_train_adasyn)[:,1]>0.07\n",
    "# Prediction for test data\n",
    "y_pred=predict_proba>0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Random Forest Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The recall score for the train data is: \", metrics.recall_score(y_train_adasyn, y_train_pred))\n",
    "print(\"The precision score for the train data is: \", metrics.precision_score(y_train_adasyn, y_train_pred))\n",
    "print(\"The recall score for the test data is: \", metrics.recall_score(y_test, y_pred))\n",
    "print(\"The precision score for the test data is: \", metrics.precision_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform cross validation & then balance classes on X_train_cv & y_train_cv using ADASYN\n",
    "\n",
    "#perform hyperparameter tuning\n",
    "\n",
    "#print the evaluation result by choosing a evaluation metric\n",
    "\n",
    "#print the optimum value of hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build models on other algorithms to see the better performing on ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the oversampling method which shows the best result on a model\n",
    "- Apply the best hyperparameter on the model\n",
    "- Predict on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform the best oversampling method on X_train & y_train\n",
    "\n",
    "clf = ___  #initialise the model with optimum hyperparameters\n",
    "clf.fit( ) # fit on the balanced dataset\n",
    "print() --> #print the evaluation score on the X_test by choosing the best evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the important features of the best model to understand the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_imp = []\n",
    "for i in clf.feature_importances_:\n",
    "    var_imp.append(i)\n",
    "print('Top var =', var_imp.index(np.sort(clf.feature_importances_)[-1])+1)\n",
    "print('2nd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-2])+1)\n",
    "print('3rd Top var =', var_imp.index(np.sort(clf.feature_importances_)[-3])+1)\n",
    "\n",
    "# Variable on Index-13 and Index-9 seems to be the top 2 variables\n",
    "top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-1])\n",
    "second_top_var_index = var_imp.index(np.sort(clf.feature_importances_)[-2])\n",
    "\n",
    "X_train_1 = X_train.to_numpy()[np.where(y_train==1.0)]\n",
    "X_train_0 = X_train.to_numpy()[np.where(y_train==0.0)]\n",
    "\n",
    "np.random.shuffle(X_train_0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "\n",
    "plt.scatter(X_train_1[:, top_var_index], X_train_1[:, second_top_var_index], label='Actual Class-1 Examples')\n",
    "plt.scatter(X_train_0[:X_train_1.shape[0], top_var_index], X_train_0[:X_train_1.shape[0], second_top_var_index],\n",
    "            label='Actual Class-0 Examples')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Print the FPR,TPR & select the best threshold from the roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train auc =', metrics.roc_auc_score(_________)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(_________)\n",
    "threshold = thresholds[np.argmax(tpr-fpr)]\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "duration": 473.597969,
   "end_time": "2021-01-03T05:26:57.795169",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-03T05:19:04.197200",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
